{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest + LIBRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers a Python-based solution to be compared to the experiments 1 to 4 presented in <cite data-cite=\"6013574/XD5B9TZQ\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented solution consists in reproducing the feature vectors for each of the experiments utilizing (x, y) points in <cite data-cite=\"6013574/XD5B9TZQ\"></cite>, applying a RandomForest classifier, and lastly, comparing the resulting performance to the ones obtained by SVM and k-NN approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob as gl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelname(file_name):  # Extract labels from filenames \n",
    "    label = file_name.replace(\"data/fundamental/sample\", \"\").lower()\n",
    "    label = label.replace(\".mat\", \"\").lower()\n",
    "    label = label.split('-', 1)[-1]\n",
    "    return label\n",
    "\n",
    "\n",
    "class Signal:  # Signal representation containing x and y coordinates and corresponding label\n",
    "    def __init__(self, x, y, label):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment utilizes each signal in its raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.1: raw data\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second experiment consists of applying z-normalization to each sample. The updated coordinates are:\n",
    "\\begin{align}\n",
    "x_{\\mathcal{N(0,1)}}=\\frac{x-\\bar{x}}{\\sigma(x)} \\\\\n",
    "y_{\\mathcal{N(0,1)}}=\\frac{y-\\bar{y}}{\\sigma(y)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.2: z-norm\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for idx, x in enumerate(sig.x):\n",
    "        sig.x[idx] = np.divide((np.transpose(x) - np.mean(x)), np.std(x))\n",
    "    for idx, y in enumerate(sig.y):\n",
    "        sig.y[idx] = np.divide((np.transpose(y) - np.mean(y)), np.std(y))\n",
    "\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third experiment consists of normalizing each signal by its 1st frame centroid, as follows:\n",
    "\\begin{align}\n",
    "\\tilde{x}_{P,f}=x_{P,f}-\\bar{x}_{1} \\\\\n",
    "\\tilde{y}_{P,f}=y_{P,f}-\\bar{y}_{1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigvalues(sig): # EX.3: norm by 1st frame centroid\n",
    "    frame_x = np.split(sig.x, 5, axis=1)  # x-coordinates by frame\n",
    "    frame_y = np.split(sig.y, 5, axis=1)  # y-coordinates by frame\n",
    "    cent_x, cent_y = (np.mean(frame_x[0], axis=1), np.mean(frame_y[0], axis=1))  # first frame centroid of each recording\n",
    "\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for idx, x in enumerate(sig.x):\n",
    "        sig.x[idx] = x - cent_x[idx]\n",
    "    for idx, y in enumerate(sig.y):\n",
    "        sig.y[idx] = y - cent_y[idx]\n",
    "\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth experiment consists of normalizing each signal by its current frame centroid, as follows:\n",
    "\\begin{align}\n",
    "\\tilde{x}_{P,f}=x_{P,f}-\\bar{x}_{f} \\\\\n",
    "\\tilde{y}_{P,f}=y_{P,f}-\\bar{y}_{f}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.4\n",
    "    frame_x = np.split(sig.x, 5, axis=1)  # x-coordinates by frame\n",
    "    frame_y = np.split(sig.y, 5, axis=1)  # y-coordinates by frame\n",
    "    cent_x, cent_y = (np.mean(frame_x, axis=2), np.mean(frame_y, axis=2))  # centroids of each recording\n",
    "\n",
    "    nframes, nrecs, idx = np.shape(frame_x)\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for fx in range(nframes):\n",
    "        frame_x[fx] = np.transpose(np.transpose(frame_x[fx]) - cent_x[fx])\n",
    "    for fy in range(nframes):\n",
    "        frame_y[fy] = np.transpose(np.transpose(frame_y[fy]) - cent_y[fy])\n",
    "\n",
    "    updsig[:, ::2] = np.hstack(frame_x)\n",
    "    updsig[:, 1::2] = np.hstack(frame_y)\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For means of comparison, this solution utilizes the Random Forest algorithm for the classification task. Tuning is done on every iteration utilizing GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = gl.glob(\"data/fundamental/*.mat\")  # type: list\n",
    "\n",
    "signals = []  # type: List[Signal]\n",
    "\n",
    "for f in files:\n",
    "    data = loadmat(f).get('pontosSinal')\n",
    "    signals.append(Signal(data[:, ::2], data[:, 1::2], labelname(f)))\n",
    "\n",
    "n_signs = len(signals)\n",
    "n_recs, n_x = np.shape(signals[0].x)  # Number of recordings and number of features\n",
    "\n",
    "signals_feat = []  # Updated signals, according to each experiment\n",
    "signals_labels = []\n",
    "labels_dict = {'angry': 1, 'disgusted': 2, 'fearful': 3, 'happy': 4, 'sad': 5, 'surprised': 6, \n",
    "               'neutral': 7, 'suckedcheeks': 8, 'inflatedcheeks': 9}  # Dictionary of signals' labels, for reference\n",
    "for s in signals:\n",
    "    signals_feat.append(sigvalues(s))\n",
    "    signals_labels.append([labels_dict[s.label]] * n_recs)\n",
    "    \n",
    "sig_features = np.reshape(signals_feat, (n_signs * n_recs, n_x * 2))\n",
    "sig_labels = np.reshape(signals_labels, (n_signs * n_recs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO: PARAMETER SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=500, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 3, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion\n",
    "criterion = ['gini', 'entropy']\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion': criterion\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv = 5\n",
    "niter = 30\n",
    "results = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_report =[]\n",
    "class_report = []\n",
    "selected_params = []\n",
    "cm = [] # confusion matrix\n",
    "feature_importance = []\n",
    "col_names = range(1,1211)\n",
    "\n",
    "for i in range(niter):\n",
    "    print(\"Iteration:: \", i)\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "    sss.get_n_splits(sig_features, sig_labels)\n",
    "\n",
    "    for train_index, test_index in sss.split(sig_features, sig_labels):\n",
    "        train_x, test_x = sig_features[train_index], sig_features[test_index]\n",
    "        train_y, test_y = sig_labels[train_index], sig_labels[test_index]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=500, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "    rf_random.fit(train_x, train_y)\n",
    "    predictions = rf_random.predict(test_x)\n",
    "    \n",
    "    selected_params.append(rf_random.best_params_)\n",
    "    train_acc.append(accuracy_score(train_y, rf_random.predict(train_x)))\n",
    "    test_acc.append(accuracy_score(test_y, predictions))\n",
    "    cm.append(pd.crosstab(test_y, predictions, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    class_report.append(classification_report(test_y, predictions, target_names=list(labels_dict.values())))\n",
    "    feature_importance.append(pd.DataFrame(data=sorted(zip(map(lambda x: round(x, 4), \n",
    "                                                               rf_random.best_estimator_.feature_importances_), \n",
    "                                                           col_names), reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "conf_matrix = reduce(lambda x, y: x.add(y, fill_value=0), cm)\n",
    "\n",
    "np.savetxt(\"results/thesis/raw-trainacc.csv\",train_acc,delimiter=\",\")\n",
    "np.savetxt(\"results/thesis/raw-testacc.csv\",test_acc,delimiter=\",\")\n",
    "np.savetxt(\"results/thesis/raw-selected_params.txt\",selected_params,fmt='%s',delimiter='\\n')\n",
    "np.savetxt(\"results/thesis/raw-classreport.txt\",class_report,fmt='%s',delimiter='\\n')\n",
    "conf_matrix.to_csv(\"results/thesis/raw-cm.csv\")\n",
    "\n",
    "import pickle\n",
    "with open('results/thesis/raw-featimportance.pickle', 'wb') as fp:\n",
    "    for feat in feature_importance:\n",
    "        pickle.dump(feat, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "acc = np.transpose([train_acc, test_acc])\n",
    "acc = pd.DataFrame(data=acc, columns=['train_acc', 'test_acc'])\n",
    "boxplot = acc.boxplot(column=['train_acc', 'test_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6013574/QDJUJ23A": {
     "URL": "http://urlib.net/8JMKD3MGPAW/3MDH39S",
     "author": [
      {
       "family": "Rezende",
       "given": "Tamires"
      },
      {
       "family": "Castro",
       "given": "Cristiano"
      },
      {
       "family": "Almeida",
       "given": "Sílvia"
      }
     ],
     "container-title": "CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI)",
     "issued": {
      "date-parts": [
       [
        2016
       ]
      ]
     },
     "title": "An approach for Brazilian Sign Language (BSL) recognition based on facial expression and k-NN classifier",
     "type": "article-journal",
     "volume": "29"
    },
    "6013574/XD5B9TZQ": {
     "URL": "https://www.ppgee.ufmg.br/defesas/1393M.PDF",
     "author": [
      {
       "family": "Rezende",
       "given": "Tamires"
      }
     ],
     "genre": "Master Thesis",
     "id": "6013574/XD5B9TZQ",
     "issued": {
      "year": 2016
     },
     "language": "Portuguese",
     "publisher": "UFMG",
     "title": "Aplicação de Técnicas de Inteligência Computacional para Análise da Expressão Facial em Reconhecimento de Sinais de Libras",
     "type": "thesis"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
